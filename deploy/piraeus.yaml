
---
kind: Namespace
apiVersion: v1
metadata:
  name: piraeus
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: piraeus-etcd
  namespace: piraeus
  labels:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-etcd
spec:
  serviceName: piraeus-etcd
  updateStrategy:
    rollingUpdate:
    type: RollingUpdate
  replicas: 3
  template:
    metadata:
      name: piraeus-etcd
      labels:
        app.kubernetes.io/name: piraeus
        app.kubernetes.io/component: piraeus-etcd
    spec:
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet      
      initContainers:
        - name: init
          image: alpine
          imagePullPolicy: Always
          resources:
            # requests:
            #   cpu: 50m
            #   memory: 50Mi
            limits:
              cpu: 100m
              memory: 100Mi
          command:
            - /bin/sh
            - -exc
            - |
              # get host ip
              SVC_IP=$( getent hosts piraeus-etcd.piraeus | awk '{print $1}' )
              HOST_IF=$( ip route get ${SVC_IP} | grep -o 'dev [^ ]*' | awk '{print $2}' )
              HOST_IP=$( ifconfig ${HOST_IF} | grep -o 'inet [^ ]*' | awk '{print $2}' | sed 's/addr://' )
              echo ${HOST_IP} > /init/hostip
          volumeMounts:
            - name: localtime
              mountPath: /etc/localtime
            - name: init
              mountPath: /init
      containers:
        - name: etcd
          image: quay.io/coreos/etcd:v3.4.1
          imagePullPolicy: Always
          resources:
            # requests:
            #   cpu: 100m
            #   memory: 100Mi
            limits:
              cpu: 500m
              memory: 500Mi
          ports:
            - name: client
              containerPort: 13379
            - name: peer
              containerPort: 13380
          env:
            - name: THIS_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ETCDCTL_ENDPOINTS
              value: piraeus-etcd.piraeus:13379
          command:
            - /bin/sh
            - -exc
            - |
              # get host ip
              HOST_IP=$( cat /init/hostip )

              # default api is v3, so add a shortcut for api v2              
              printf "#!/bin/sh\nexport ETCDCTL_API=2\nexport ETCDCTL_ENDPOINTS=http://${HOST_IP}:13379\netcdctl \$@" > /usr/local/bin/etcdctl2
              chmod +x /usr/local/bin/etcdctl2

              printf "#!/bin/sh\nexport ETCDCTL_API=3\nexport ETCDCTL_ENDPOINTS=http://${HOST_IP}:13379\netcdctl \$@" > /usr/local/bin/etcdctl3
              chmod +x /usr/local/bin/etcdctl3

              # add to cluster if necessary
              
              if [ "${THIS_POD_NAME}" = "piraeus-etcd-0" ] && ! etcdctl endpoint status; then
                INI_STATE=new
              else
                INI_STATE=existing
                EXISTING_CLUSTER=$(  etcdctl member list | sed 's/,//g' |  awk '{print ","$3"="$4}' )
                etcdctl member add etcd-${HOST_IP} --peer-urls=http://${HOST_IP}:13380 || true 
              fi
              
              # start etcd
              exec etcd --name etcd-${HOST_IP} \
                --max-txn-ops 1024 \
                --listen-peer-urls http://${HOST_IP}:13380 \
                --listen-client-urls http://${HOST_IP}:13379 \
                --advertise-client-urls http://${HOST_IP}:13379 \
                --initial-advertise-peer-urls http://${HOST_IP}:13380 \
                --initial-cluster-token piraeus-etcd-cluster \
                --initial-cluster "etcd-${HOST_IP}=http://${HOST_IP}:13380${EXISTING_CLUSTER}" \
                --initial-cluster-state ${INI_STATE} \
                --data-dir /var/run/etcd/data \
                --enable-v2 
          lifecycle:
              preStop:
                exec:
                  command:
                    - /bin/sh
                    - -exc
                    - |
                      MEMBER_ID=$( etcdctl endpoint status | awk '{print $2}' | sed 's/,//g' )
                      etcdctl member remove ${MEMBER_ID} || true
                      rm -fr /var/run/etcd/*
          readinessProbe:
            successThreshold: 3
            failureThreshold: 3
            httpGet:
              path: /health
              port: 13379
            initialDelaySeconds: 10
            periodSeconds: 1
          volumeMounts:
            - name: localtime
              mountPath: /etc/localtime
            - name: init
              mountPath: /init
            - name: data
              mountPath: /var/run/etcd
      volumes:
        - name: localtime
          hostPath:
            path: /etc/localtime
        - name: init
          emptyDir: {}
        - name: data
          # emptyDir: {}
          hostPath:
            path: /var/local/piraeus/etcd
      imagePullSecrets:
        - name: piraeus-regcred
        # tolerations:
        #   - operator: "Exists"
        #     effect: "NoSchedule"
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - piraeus-etcd
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: piraeus/etcd
                  operator: In
                  values:
                    - "true"
                # - key: node-role.kubernetes.io/master
                #   operator: DoesNotExist
---
apiVersion: v1
kind: Service
metadata:
  name: piraeus-etcd
  namespace: piraeus
  labels:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-etcd
  annotations:
    # Create endpoints also if the related pod isn't ready
    
spec:
  type: ClusterIP
  ports:
    - port: 13379
      name: client
      targetPort: 13379
    - port: 13380
      name: peer
      targetPort: 13380
  selector:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-etcd

---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: piraeus-controller
  namespace: piraeus
  labels:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-controller
spec:
  serviceName: piraeus-controller
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: piraeus
      app.kubernetes.io/component: piraeus-controller
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: piraeus
        app.kubernetes.io/component: piraeus-controller
    spec:
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      terminationGracePeriodSeconds: 0
      # hostNetwork: true
      # dnsPolicy: ClusterFirstWithHostNet
      dnsPolicy: ClusterFirst
      # imagePullSecrets:
      # - name: piraeus-regcred
      initContainers:
      - name: wait-for-etcd
        image: alexzhc/piraeus-server:v1.2.1
        imagePullPolicy: Always
        resources:
          # requests:
          #   cpu: 50m
           #   memory: 50Mi
          limits:
            cpu: 100m
            memory: 100Mi
        env:
        - name: ETCD_URL
          value: 'http://piraeus-etcd.piraeus:13379'
        - name: TIMEOUT
          value: '3600'
        command:
        - /bin/bash
        - -xc
        - |
          # Check if etcd is health  
          SECONDS=0
          until curl -sS ${ETCD_URL}/health --connect-timeout 2 | grep -w true; do
            sleep 2
            if [ "${SECONDS}" -ge  "${TIMEOUT}" ]; then
              echo ERR: Unable to reach controller 
              exit 0 # 
            fi
          done
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime 
      containers:
      - name: piraeus-controller
        image: alexzhc/piraeus-server:v1.2.1
        imagePullPolicy: Always
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: '1'
            memory: '1Gi'
        ports:
        - name: rest-api
          containerPort: 3370
          hostPort: 3370
        - name: plain
          containerPort: 3376
          hostPort: 3376
        - name: ssl
          containerPort: 3377
          hostPort: 3377
        env:
        - name: ETCD_URL
          value: 'http://piraeus-etcd.piraeus:13379'
        - name: ETCDCTL_ENDPOINTS
          value: 'http://piraeus-etcd.piraeus:13379'
        - name: ETCDCTL_API
          value: '3'
        command:
        - /bin/bash
        - -exc
        - |
          ETCD_CLUSTER="etcd://"$( etcdctl member list | sed 's/,//g' |  awk '{print $5}' | paste -d, - - - )
          sed -i "s#jdbc:h2:/var/lib/linstor/linstordb#${ETCD_CLUSTER}#" /etc/linstor/linstor.toml
          cat /etc/linstor/linstor.toml
          
          /usr/share/linstor-server/bin/Controller \
          --logs=/var/log/linstor-controller \
          --config-directory=/etc/linstor
        readinessProbe:
          successThreshold: 3
          failureThreshold: 3
          httpGet:
            port: 3370
          initialDelaySeconds: 5
          periodSeconds: 1
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime
        - name: log
          mountPath: /var/log/linstor-controller
      volumes:
      - name: localtime
        hostPath:
          path: /etc/localtime
      - name: log
        hostPath:
          path: /var/log/piraeus/linstor-controller
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
                # - key: piraeus/controller
                #   operator: In
                #   values:
                #   - "true"
              - key: node-role.kubernetes.io/master
                operator: DoesNotExist
      # tolerations:
      #   - operator: "Exists"
      #     effect: "NoSchedule"
---
apiVersion: v1
kind: Service
metadata:
  name: piraeus-controller
  namespace: piraeus
  labels:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-controller
spec:
  type: ClusterIP
  ports:
  - port: 3370
    name: rest-api
    targetPort: 3370
  selector:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-controller
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: piraeus-node
  namespace: piraeus
spec:
  minReadySeconds: 0
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: piraeus
        app.kubernetes.io/component: piraeus-node
    spec:
      priorityClassName: system-node-critical
      restartPolicy: Always
      terminationGracePeriodSeconds: 0
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      # imagePullSecrets:
      # - name: piraeus-regcred
      initContainers:
      - name: register-node
        image: alexzhc/piraeus-server:v1.2.1
        imagePullPolicy: Always
        resources:
          # requests:
          #   cpu: 100m
           #   memory: 100Mi
          limits:
            cpu: 500m
            memory: 500Mi
        env:
        - name: LS_CONTROLLERS
          value: 'piraeus-controller.piraeus:3370'
        - name: TIMEOUT
          value: '3600'
        command:
        - /bin/bash
        - -xc
        - |
          # Check if controller is up  
          SECONDS=0
          until curl -sS ${LS_CONTROLLERS} --connect-timeout 2 ; do
            sleep 0.5
            if [ "${SECONDS}" -ge  "${TIMEOUT}" ]; then
                echo ERR: Unable to reach controller 
                exit 0 # Don't block node from coming up
            fi
          done

          # Obtain node ip and interface
          CONTROLLER_IP=$( getent hosts piraeus-controller.piraeus | awk '{print $1}' )
          HOST_IF=$( ip route get ${CONTROLLER_IP} | grep -o 'dev [^ ]*' | awk '{print $2}' )
          HOST_IP=$( ifconfig ${HOST_IF} | grep -o 'inet [^ ]*' | awk '{print $2}' )

          # Register node to controller by "Best-Effort"
          if linstor node list -p | tail -n+3 | grep -w "${HOSTNAME}\|${HOST_IP}"; then
              echo WARN: This node is already the cluster
              linstor node list -p 
          elif linstor node create ${HOSTNAME} ${HOST_IP} --node-type Satellite --interface-name ${HOST_IF}; then
              echo INFO: Successfully added this node to the cluster
              linstor node list -p -N ${HOSTNAME}  
          else 
              echo ERR: Failed to add this node to the cluster
              linstor node list -p 
          fi

          exit 0 # Don't block node from coming up
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime 
        - name: usr-sbin
          mountPath: /install/usr/sbin

      - name: load-drbd
        image: alexzhc/drbd-builder:9.0.21-centos
        imagePullPolicy: Always
        resources:
          # requests:
          #   cpu: '500m'
           #   memory: '500Mi'
          limits:
            cpu: 1
            memory: 1Gi
        env:
        - name: DO_LOAD_DRBD
          value: 'TRUE'
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime
        - name: usr-src-kernels
          mountPath: /usr/src/kernels 
        - name: lib-modules
          mountPath: /lib/modules    
        - name: etc
          mountPath: /install/etc
        - name: lib 
          mountPath: /install/lib
        - name: usr
          mountPath: /install/usr
        - name: var
          mountPath: /install/var 

      containers:
      - name: piraeus-satellite
        image: alexzhc/piraeus-server:v1.2.1
        imagePullPolicy: Always
        securityContext:
          privileged: true
        resources:
          # requests:
          #   cpu: '500m'
           #   memory: '500Mi'
          limits:
            cpu: 1
            memory: 1Gi
        env:
        - name: LS_CONTROLLERS
          value: piraeus-controller.piraeus:3370
        - name: DfltStorPool_Dir
          value: /var/local/piraeus/pools/DfltStorPool
        command:
        - /usr/share/linstor-server/bin/Satellite
        - --logs=/var/log/linstor-satellite/
        - --config-directory=/etc/linstor 
        readinessProbe:
          successThreshold: 3
          failureThreshold: 3
          tcpSocket:
            port: 3366
          initialDelaySeconds: 5
          periodSeconds: 1
        lifecycle:
          postStart:
            exec:
              command: 
              - /bin/bash
              - -xc
              - |       
                # load drbd modules
                grep '^drbd ' /proc/modules || modinfo drbd && modprobe -v drbd
                grep '^drbd_transport_tcp ' /proc/modules || modinfo drbd_transport_tcp && modprobe -v drbd_transport_tcp

                # wait until node process is up
                until nc -zvw2 127.0.0.1 3366 ; do
                  sleep 0.5 
                  echo ${SECONDS} >> /tmp/poststart.log
                  if [ "${SECONDS}" -ge 60 ]; then
                      echo ERR: Unable to reach satellite
                      exit 0 # Don't block node readiness
                  fi
                done

                # wait until node is online
                until linstor node list -p | tee -a /tmp/poststart.log | grep -w "${HOSTNAME}" | grep -w Online; do
                  sleep 2
                  echo ${SECONDS} >> /tmp/poststart.log
                  if [ "${SECONDS}" -ge 60 ]; then
                      echo ERR: Satellite online-check timed out >> /tmp/poststart.log
                      exit 0 # Don't block node readiness
                  fi
                done

                [ -d ${DfltStorPool_Dir} ] || mkdir -vp ${DfltStorPool_Dir}
                if linstor storage-pool list -p | tee -a /tmp/poststart.log | grep -w "${HOSTNAME}" | grep -w DfltStorPool; then 
                  exit 0 
                else 
                  linstor storage-pool create filethin ${HOSTNAME} DfltStorPool ${DfltStorPool_Dir} >> /tmp/poststart.log 2>&1 
                fi 

                exit 0 # Don't block node readiness 
        volumeMounts:        
        - name: localtime
          mountPath: /etc/localtime
        - name: log
          mountPath: /var/log/linstor-satellite
        - name: var-local-piraeus
          mountPath: /var/local/piraeus
        - name: dev
          mountPath: /dev
        - name: lib-modules
          mountPath: /lib/modules
        - name: etc-lvm
          mountPath: /etc/lvm    
        - name: run-lvm
          mountPath: /run/lvm
        - name: etc-drbd-d
          mountPath: /etc/drbd.d
        - name: var-lib-linstor-d
          mountPath: /var/lib/linstor.d
        - name: etc-linstor
          mountPath: /etc/linstor

      volumes:
      # Host mounts for operaton container
      - name: localtime
        hostPath:
          path: /etc/localtime
      - name: log
        hostPath: 
          path: /var/log/piraeus/linstor-satellite
      - name: var-local-piraeus
        hostPath:
          path: /var/local/piraeus
      - name: dev
        hostPath: 
          path: /dev
      - name: usr-sbin
        hostPath:
          path: /usr/sbin
      - name: lib-modules
        hostPath: 
          path: /lib/modules
      - name: etc-lvm
        hostPath: 
          path: /etc/lvm
      - name: run-lvm
        hostPath: 
          path: /run/lvm
      - name: etc-drbd-d
        hostPath: 
          path: /etc/drbd.d
      - name: var-lib-linstor-d
        hostPath: 
          path: /var/lib/linstor.d
      - name: etc-linstor
        hostPath: 
          path: /etc/linstor

      # Host mounts for drop installation
      - name: usr-src-kernels
        hostPath: 
          path: /usr/src/kernels
      - name: etc
        hostPath: 
          path: /etc
      - name: lib
        hostPath: 
          path: /lib
      - name: usr
        hostPath:
          path: /usr
      - name: var
        hostPath: 
          path: /var          

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: piraeus/node
                operator: In
                values:
                - "true"
              - key: node-role.kubernetes.io/master
                operator: DoesNotExist
      tolerations:
        - operator: "Exists"
          effect: "NoSchedule"
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: piraeus-csi-controller
  namespace: piraeus
  labels:
    app.kubernetes.io/name: piraeus
    app.kubernetes.io/component: piraeus-csi-controller
spec:
  serviceName: piraeus-csi-controller
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: piraeus
      app.kubernetes.io/component: piraeus-csi-controller
  replicas: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: piraeus
        app.kubernetes.io/component: piraeus-csi-controller
    spec:
      serviceAccount: piraeus-csi-controller-sa
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      terminationGracePeriodSeconds: 0
      # hostNetwork: true
      # dnsPolicy: ClusterFirstWithHostNet
      dnsPolicy: ClusterFirst
      # imagePullSecrets:
      # - name: piraeus-regcred
      containers:
      - name: csi-provisioner
        image: piraeus/csi-provisioner:v1.2.0
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
        - "--csi-address=$(ADDRESS)"
        - "--v=5"
        - "--feature-gates=Topology=true"
        - "--timeout=120s"
        env:
        - name: ADDRESS
          value: /var/lib/csi/sockets/pluginproxy/csi.sock
        volumeMounts:
        - name: socket-dir
          mountPath: /var/lib/csi/sockets/pluginproxy/
      - name: csi-snapshotter
        image: piraeus/csi-snapshotter:v1.1.0
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
        - "--csi-address=$(ADDRESS)"
        - "--timeout=120s"
        env:
        - name: ADDRESS
          value: /var/lib/csi/sockets/pluginproxy/csi.sock
        volumeMounts:
        - name: socket-dir
          mountPath: /var/lib/csi/sockets/pluginproxy/
      - name: csi-cluster-driver-registrar
        image: piraeus/csi-cluster-driver-registrar:v1.0.1
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
        - "--v=5"
        - "--pod-info-mount-version=\"v1\""
        - "--csi-address=$(ADDRESS)"
        env:
        - name: ADDRESS
          value: /var/lib/csi/sockets/pluginproxy/csi.sock
        volumeMounts:
        - name: socket-dir
          mountPath: /var/lib/csi/sockets/pluginproxy/
      - name: csi-attacher
        image: piraeus/csi-attacher:v1.1.1
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
        - "--v=5"
        - "--csi-address=$(ADDRESS)"
        - "--timeout=120s"
        env:
        - name: ADDRESS
          value: /var/lib/csi/sockets/pluginproxy/csi.sock
        volumeMounts:
        - name: socket-dir
          mountPath: /var/lib/csi/sockets/pluginproxy/
      - name: piraeus-csi-plugin
        image: piraeus/piraeus-csi:v0.7.2
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
          - "--csi-endpoint=$(CSI_ENDPOINT)"
          - "--node=$(KUBE_NODE_NAME)"
          - "--linstor-endpoint=$(LS_CONTROLLERS)"
          - "--log-level=debug"
        env:
          - name: CSI_ENDPOINT
            value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: LS_CONTROLLERS
            value: "http://piraeus-controller.piraeus:3370"
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime
        - name: socket-dir
          mountPath: /var/lib/csi/sockets/pluginproxy/

      volumes:
      - name: localtime
        hostPath:
          path: /etc/localtime
      - name: socket-dir
        emptyDir: {}
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #       - matchExpressions:
      #        - key: piraeus/controller
      #          operator: In
      #          values:
      #          - "true"
      #         - key: node-role.kubernetes.io/master
      #           operator: Exists
      # tolerations:
      #   - operator: "Exists"
      #     effect: "NoSchedule"
---
kind: ServiceAccount
apiVersion: v1
metadata:
  name: piraeus-csi-controller-sa
  namespace: piraeus
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-provisioner-role
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "list"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["list", "watch", "create", "update", "patch"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshots"]
    verbs: ["get", "list"]
  - apiGroups: ["snapshot.storage.k8s.io"]
    resources: ["volumesnapshotcontents"]
    verbs: ["get", "list"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-provisioner-binding
subjects:
  - kind: ServiceAccount
    name: piraeus-csi-controller-sa
    namespace: piraeus
roleRef:
  kind: ClusterRole
  name: csi-provisioner-role
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-attacher-role
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["csinodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["volumeattachments"]
    verbs: ["get", "list", "watch", "update"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-attacher-binding
subjects:
  - kind: ServiceAccount
    name: piraeus-csi-controller-sa
    namespace: piraeus
roleRef:
  kind: ClusterRole
  name: csi-attacher-role
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-cluster-driver-registrar-role
rules:
- apiGroups: ["csi.storage.k8s.io"]
  resources: ["csidrivers"]
  verbs: ["create", "delete", "list"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["create", "delete"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-cluster-driver-registrar-binding
subjects:
  - kind: ServiceAccount
    name: piraeus-csi-controller-sa
    namespace: piraeus
roleRef:
  kind: ClusterRole
  name: csi-cluster-driver-registrar-role
  apiGroup: rbac.authorization.k8s.io 
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: piraeus-csi-node
  namespace: piraeus
spec:
  minReadySeconds: 0
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: piraeus
        app.kubernetes.io/component: piraeus-csi-node
    spec:
      priorityClassName: system-node-critical
      serviceAccount: piraeus-csi-node-sa
      restartPolicy: Always
      terminationGracePeriodSeconds: 0
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      # imagePullSecrets:
      # - name: piraeus-regcred
      containers:
      - name: csi-node-driver-registrar
        image: piraeus/csi-node-driver-registrar:v1.1.0
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
        - "--v=5"
        - "--csi-address=$(ADDRESS)"
        - "--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)"
        lifecycle:
            preStop:
              exec:
                command: ["/bin/sh", "-c", "rm -rf /registration/linstor.csi.linbit.com /registration/linstor.csi.linbit.com-reg.sock"]
        env:
        - name: ADDRESS
          value: /csi/csi.sock
        - name: DRIVER_REG_SOCK_PATH
          value: /var/lib/kubelet/plugins/linstor.csi.linbit.com/csi.sock
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        volumeMounts:
        - name: plugin-dir
          mountPath: /csi/
        - name: registration-dir
          mountPath: /registration/
      - name: piraeus-csi-plugin
        image: piraeus/piraeus-csi:v0.7.2
        imagePullPolicy: "Always"
        resources:
          # requests:
          #   cpu: '500m'
          #   memory: '500Mi'
          limits:
            cpu: 500m
            memory: 500Mi
        args:
        - "--csi-endpoint=$(CSI_ENDPOINT)"
        - "--node=$(KUBE_NODE_NAME)"
        - "--linstor-endpoint=$(LS_CONTROLLERS)"
        - "--log-level=debug"
        env:
        - name: CSI_ENDPOINT
          value: unix:///csi/csi.sock
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: LS_CONTROLLERS
          value: "http://piraeus-controller.piraeus:3370"
        securityContext:
          privileged: true
          capabilities:
            add: ["SYS_ADMIN"]
          allowPrivilegeEscalation: true
        volumeMounts:
        - name: localtime
          mountPath: /etc/localtime
        - name: plugin-dir
          mountPath: /csi
        - name: pods-mount-dir
          mountPath: /var/lib/kubelet
          mountPropagation: "Bidirectional"
        - name: device-dir
          mountPath: /dev

      volumes:
      - name: localtime
        hostPath:
          path: /etc/localtime
      - name: registration-dir
        hostPath:
          path: /var/lib/kubelet/plugins_registry/
          type: DirectoryOrCreate
      - name: plugin-dir
        hostPath:
          path: /var/lib/kubelet/plugins/linstor.csi.linbit.com/
          type: DirectoryOrCreate
      - name: pods-mount-dir
        hostPath:
          path: /var/lib/kubelet
          type: Directory
      - name: device-dir
        hostPath:
          path: /dev
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: piraeus/node
                operator: In
                values:
                - "true"
              # - key: node-role.kubernetes.io/master
              #   operator: DoesNotExist
      tolerations:
        - operator: "Exists"
          effect: "NoSchedule"          
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: piraeus-csi-node-sa
  namespace: piraeus
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-driver-registrar-role
  namespace: piraeus
rules:
- apiGroups: [""]
  resources: ["events"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-driver-registrar-binding
subjects:
- kind: ServiceAccount
  name: piraeus-csi-node-sa
  namespace: piraeus
roleRef:
  kind: ClusterRole
  name: csi-driver-registrar-role
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-snapshotter-role
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["events"]
  verbs: ["list", "watch", "create", "update", "patch"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshotclasses"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshotcontents"]
  verbs: ["create", "get", "list", "watch", "update", "delete"]
- apiGroups: ["snapshot.storage.k8s.io"]
  resources: ["volumesnapshots"]
  verbs: ["get", "list", "watch", "update"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["create", "list", "watch", "delete"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: csi-snapshotter-binding
subjects:
- kind: ServiceAccount
  name: piraeus-controller-sa
  namespace: piraeus
roleRef:
  kind: ClusterRole
  name: csi-snapshotter-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: piraeus-default-r1
provisioner: linstor.csi.linbit.com
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  placementCount: "1"
  placementPolicy: AutoPlace
  allowRemoteVolumeAccess: "true"
  disklessOnRemaining: "false"
  fs: ext4
  mountOpts: noatime,discard
  storagePool: DfltStorPool
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: piraeus-default-r2
provisioner: linstor.csi.linbit.com
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  placementCount: "2"
  placementPolicy: AutoPlace
  allowRemoteVolumeAccess: "true"
  disklessOnRemaining: "false"
  fs: ext4
  mountOpts: noatime,discard
  storagePool: DfltStorPool
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: piraeus-default-r3
provisioner: linstor.csi.linbit.com
allowVolumeExpansion: true
reclaimPolicy: Delete
parameters:
  placementCount: "3"
  placementPolicy: AutoPlace
  allowRemoteVolumeAccess: "true"
  disklessOnRemaining: "false"
  fs: ext4
  mountOpts: noatime,discard
  storagePool: DfltStorPool